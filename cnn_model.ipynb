{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.25\n",
    "batch_size = 128\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove.6B.100d.txt\n",
      "Total 400000 word vectors collected in the dictionary\n"
     ]
    }
   ],
   "source": [
    "word2vec = {}\n",
    "with open(f'glove.6B.{EMBEDDING_DIM}d.txt', encoding='utf-8') as f:\n",
    "    print(f.name)\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0]\n",
    "        vector = np.asarray(parts[1:], dtype='float32')\n",
    "        word2vec[word] = vector\n",
    "print('Total {} word vectors collected in the dictionary'.format(len(word2vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
       "       'insult', 'identity_hate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               0\n",
       "comment_text     0\n",
       "toxic            0\n",
       "severe_toxic     0\n",
       "obscene          0\n",
       "threat           0\n",
       "insult           0\n",
       "identity_hate    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = train_df['comment_text'].fillna('NO_COMMENT').values\n",
    "label_list = ['toxic', 'severe_toxic', 'obscene', 'threat',\n",
    "       'insult', 'identity_hate']\n",
    "targets = train_df[label_list].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 1400\n",
      "Minimum sequence length: 0\n",
      "Total 210337 unique tokens found\n",
      "Shape of word matrix: (159571, 100)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(comments)\n",
    "sequences = tokenizer.texts_to_sequences(comments)\n",
    "\n",
    "print('Maximum sequence length: {}'.format(max(len(seq) for seq in sequences)))\n",
    "print('Minimum sequence length: {}'.format(min(len(seq) for seq in sequences)))\n",
    "\n",
    "word2index = tokenizer.word_index\n",
    "print('Total {} unique tokens found'.format(len(word2index)))\n",
    "\n",
    "word_matrix = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of word matrix: {}'.format(word_matrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = min(MAX_VOCAB_SIZE, len(word2index)+1)\n",
    "embedding_matrix = np.zeros((word_count, EMBEDDING_DIM))\n",
    "for word, index in word2index.items():\n",
    "    if index < MAX_VOCAB_SIZE:\n",
    "        vector = word2vec.get(word)\n",
    "        if vector is not None:\n",
    "            embedding_matrix[index]=vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Input, Model\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Dense, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(word_count, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False, input_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 100)          2000000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 98, 128)           38528     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 30, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 8, 128)            49280     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 2,154,374\n",
      "Trainable params: 154,374\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_comment = Input((MAX_SEQUENCE_LENGTH,))\n",
    "x = embedding_layer(input_comment)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(128, 3 , activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output = Dense(len(label_list), activation='sigmoid')(x)\n",
    "\n",
    "model = Model(input_comment, output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 119678 samples, validate on 39893 samples\n",
      "Epoch 1/20\n",
      "119678/119678 [==============================] - 19s 160us/step - loss: 0.0856 - accuracy: 0.9725 - val_loss: 0.0721 - val_accuracy: 0.9754\n",
      "Epoch 2/20\n",
      "119678/119678 [==============================] - 12s 96us/step - loss: 0.0684 - accuracy: 0.9772 - val_loss: 0.0724 - val_accuracy: 0.9770\n",
      "Epoch 3/20\n",
      "119678/119678 [==============================] - 13s 105us/step - loss: 0.0636 - accuracy: 0.9784 - val_loss: 0.0677 - val_accuracy: 0.9774\n",
      "Epoch 4/20\n",
      "119678/119678 [==============================] - 11s 91us/step - loss: 0.0606 - accuracy: 0.9791 - val_loss: 0.0766 - val_accuracy: 0.9763\n",
      "Epoch 5/20\n",
      "119678/119678 [==============================] - 12s 101us/step - loss: 0.0582 - accuracy: 0.9798 - val_loss: 0.0695 - val_accuracy: 0.9775\n",
      "Epoch 6/20\n",
      "119678/119678 [==============================] - 13s 108us/step - loss: 0.0561 - accuracy: 0.9803 - val_loss: 0.0690 - val_accuracy: 0.9774\n",
      "Epoch 7/20\n",
      "119678/119678 [==============================] - 12s 97us/step - loss: 0.0544 - accuracy: 0.9809 - val_loss: 0.0696 - val_accuracy: 0.9774\n",
      "Epoch 8/20\n",
      "119678/119678 [==============================] - 12s 98us/step - loss: 0.0525 - accuracy: 0.9812 - val_loss: 0.0726 - val_accuracy: 0.9773\n",
      "Epoch 9/20\n",
      "119678/119678 [==============================] - 11s 92us/step - loss: 0.0511 - accuracy: 0.9817 - val_loss: 0.0793 - val_accuracy: 0.9772\n",
      "Epoch 10/20\n",
      "119678/119678 [==============================] - 12s 98us/step - loss: 0.0497 - accuracy: 0.9821 - val_loss: 0.0883 - val_accuracy: 0.9768\n",
      "Epoch 11/20\n",
      "119678/119678 [==============================] - 11s 92us/step - loss: 0.0488 - accuracy: 0.9826 - val_loss: 0.0806 - val_accuracy: 0.9769\n",
      "Epoch 12/20\n",
      "119678/119678 [==============================] - 12s 98us/step - loss: 0.0479 - accuracy: 0.9826 - val_loss: 0.0901 - val_accuracy: 0.9743\n",
      "Epoch 13/20\n",
      "119678/119678 [==============================] - 13s 106us/step - loss: 0.0467 - accuracy: 0.9830 - val_loss: 0.1232 - val_accuracy: 0.9765\n",
      "Epoch 14/20\n",
      "119678/119678 [==============================] - 13s 108us/step - loss: 0.0465 - accuracy: 0.9833 - val_loss: 0.0936 - val_accuracy: 0.9764\n",
      "Epoch 15/20\n",
      "119678/119678 [==============================] - 12s 99us/step - loss: 0.0452 - accuracy: 0.9835 - val_loss: 0.1363 - val_accuracy: 0.9766\n",
      "Epoch 16/20\n",
      "119678/119678 [==============================] - 11s 92us/step - loss: 0.0444 - accuracy: 0.9838 - val_loss: 0.1526 - val_accuracy: 0.9765\n",
      "Epoch 17/20\n",
      "119678/119678 [==============================] - 11s 94us/step - loss: 0.0437 - accuracy: 0.9839 - val_loss: 0.1637 - val_accuracy: 0.9734\n",
      "Epoch 18/20\n",
      "119678/119678 [==============================] - 11s 91us/step - loss: 0.0432 - accuracy: 0.9843 - val_loss: 0.1430 - val_accuracy: 0.9763\n",
      "Epoch 19/20\n",
      "119678/119678 [==============================] - 11s 91us/step - loss: 0.0420 - accuracy: 0.9844 - val_loss: 0.2264 - val_accuracy: 0.9753\n",
      "Epoch 20/20\n",
      "119678/119678 [==============================] - 11s 91us/step - loss: 0.0422 - accuracy: 0.9845 - val_loss: 0.1821 - val_accuracy: 0.9766\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(word_matrix, targets, batch_size=batch_size, epochs=epochs, validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
